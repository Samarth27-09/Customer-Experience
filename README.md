# Fulfillment & Customer Experience Analytics

This repository contains a comprehensive data analytics project aimed at diagnosing and improving customer support operations. By transforming raw customer support ticket data into actionable business intelligence, this project provides a strategic framework for enhancing customer satisfaction, streamlining fulfillment processes, and boosting operational efficiency. The analysis focuses on identifying the root causes of customer dissatisfaction, benchmarking agent and shift performance, and uncovering systemic bottlenecks.

-----

## üéØ Project Objectives

The core objectives of this analysis are to provide data-driven answers to critical business questions:

  * **Root-Cause Analysis**: To systematically identify, categorize, and quantify the primary drivers of customer complaints by applying text analysis techniques to ticket descriptions. The goal is to move beyond anecdotal evidence to a statistical understanding of user pain points.
  * **Performance Benchmarking**: To establish and monitor key performance indicators (KPIs) such as Customer Satisfaction (CSAT), Time to Resolution (TTR), and First Response Time (FRT). This involves creating performance baselines to evaluate agents, shifts, and issue categories against a consistent standard.
  * **Operational Bottleneck Detection**: To diagnose inefficiencies within the support workflow, with a specific focus on identifying patterns in night-shift operations and the performance trajectory of newly hired agents. The objective is to pinpoint areas where process improvements or additional training can yield the highest impact.

-----

## üíæ Dataset

The analysis utilizes the **Customer Support Ticket Dataset**, a collection of anonymized support interactions.

This dataset provides a granular view of the customer support lifecycle. Each record represents a single support ticket and includes both structured metadata and unstructured text.

**Key Dataset Columns:**

| Column Name | Description | Data Type |
| :--- | :--- | :--- |
| `TicketID` | Unique identifier for each support ticket. | String |
| `TicketType` | The category of the support request (e.g., "Billing", "Technical"). | String |
| `TicketDescription` | Unstructured text field containing the customer's original query. | String |
| `Resolution` | Unstructured text field describing the final resolution provided. | String |
| `CreationTimestamp` | Timestamp when the ticket was created. | Datetime |
| `FirstResponseTimestamp` | Timestamp of the first agent response. | Datetime |
| `ResolutionTimestamp` | Timestamp when the ticket was marked as resolved. | Datetime |
| `CustomerSatisfactionRating`| Customer-provided rating on a scale of 1-5. | Integer |
| `TicketChannel` | The channel through which the ticket was submitted (e.g., "Email", "Web"). | String |
| `AgentID` | Unique identifier for the handling agent. | String |
| `Shift` | The shift during which the ticket was handled (e.g., "Day", "Night"). | String |

-----

## üîë Key Tables Used

The analysis relies on a primary data table and two derivative tables created during the workflow.

| Table Name | Description |
| :--- | :--- |
| `support_tickets` | The main source table containing all raw, unmodified ticket data as described above. |
| `issue_keywords` | A derivative table generated via Python scripts. It maps `TicketID`s to significant keywords extracted from the `TicketDescription`. This table powers the root-cause analysis by linking specific terms to performance metrics like CSAT. |
| `csat_benchmark` | An aggregated metrics table generated by SQL queries. It contains pre-calculated KPIs (CSAT, TTR, FRT) grouped by `AgentID`, `Shift`, and `TicketType`. This serves as the clean, primary data source for the Power BI dashboard. |

-----

## üõ†Ô∏è Tools Used

  * **SQL**: Used for its efficiency in handling large-scale data aggregation. The analysis involved complex `JOIN`s, `GROUP BY` clauses, and window functions to calculate metrics across different dimensions from the relational database.
  * **Python (Pandas, NLTK)**: Employed for all data wrangling and text analysis tasks. The **Pandas** library was essential for data cleaning and transformation, while **NLTK** (Natural Language Toolkit) was used for tokenization, stop-word removal, and frequency distribution during the keyword analysis phase.
  * **Power BI**: Selected as the visualization tool for its powerful data modeling capabilities and interactive features. **DAX (Data Analysis Expressions)** was used to create complex custom measures, and the dashboard's drill-through and cross-filtering functionalities allow stakeholders to seamlessly explore the data from a high-level overview down to individual ticket details.

-----

## üîÑ Project Workflow

The project was executed in a sequential workflow, from raw data ingestion to final visualization.

1.  **Data Cleaning and Preprocessing**: The initial phase involved rigorous data cleaning using Python. This included handling `NULL` values in `ResolutionTimestamp` and `CustomerSatisfactionRating`, standardizing datetime formats to ensure accurate time-difference calculations, and removing duplicate ticket entries.
2.  **Keyword Mapping & Text Analysis**: A text mining script was developed to parse the `TicketDescription` field. The process involved tokenizing the text, removing common English stop words and punctuation, and performing a frequency analysis to identify the most common keywords associated with tickets having low CSAT scores (1 or 2).
3.  **SQL-based Aggregations and Metric Calculation**: SQL scripts were executed to transform the cleaned transactional data into an aggregated metrics table (`csat_benchmark`). These queries calculated average CSAT, TTR (as `ResolutionTimestamp - CreationTimestamp`), and FRT (as `FirstResponseTimestamp - CreationTimestamp`) for each agent, shift, and ticket category.
4.  **Dashboarding in Power BI**: The aggregated SQL table was connected to Power BI. A multi-page interactive dashboard was designed. The main page features KPI cards for a high-level summary, while other pages provide detailed breakdowns of agent performance, shift comparisons, and a deep dive into complaint keywords.

-----

## üìä Key Metrics

The following KPIs were central to the analysis, each defined by a specific calculation:

  * **CSAT (Customer Satisfaction Rating)**: The mean of all `CustomerSatisfactionRating` values.
      * *Formula: `AVG(CustomerSatisfactionRating)`*
  * **TTR (Time to Resolution)**: The average duration between ticket creation and its resolution.
      * *Formula: `AVG(ResolutionTimestamp - CreationTimestamp)`*
  * **FRT (First Response Time)**: The average duration between ticket creation and the first agent contact.
      * *Formula: `AVG(FirstResponseTimestamp - CreationTimestamp)`*
  * **Complaint Volume by Type/Keyword**: A count of tickets associated with specific negative keywords or categories.
      * *Formula: `COUNT(TicketID) WHERE Keyword IN ('keyword1', 'keyword2')`*
  * **Agent Performance**: A composite view combining the above metrics, segmented by agent, shift, and experience level.

-----

## üî¨ Analysis

The analytical approach was structured to address the core project objectives through distinct methodologies.

  * **Root-Cause Analysis**: To identify the drivers of poor customer experiences, the analysis isolated all tickets with a CSAT rating of 1 or 2. The associated `TicketDescription` text was tokenized, and a Term Frequency analysis was conducted to surface the most common nouns and noun phrases. This quantitative approach replaced subjective assessments with a data-backed list of top customer pain points.
  * **Performance Benchmarking**: Statistical baselines were created by calculating the mean and standard deviation for CSAT, TTR, and FRT across the entire dataset. Each agent, shift, and issue category was then compared against these benchmarks. Performance was often measured in terms of standard deviations from the mean to identify statistically significant outliers.
  * **Bottleneck Identification**: The analysis of operational bottlenecks involved creating time-series plots of TTR and ticket volume for day vs. night shifts. For new hires, TTR was plotted against agent tenure (in months) to model the learning curve. Agents or shifts with a consistently high TTR despite an average or low ticket volume were flagged as potential bottlenecks requiring further investigation.

-----

## üí° Insights

The analysis produced several actionable insights with direct implications for business strategy:

  * **Primary Complaint Drivers Identified**: The text analysis conclusively showed that keywords "billing error," "renewal," and "slow performance" were present in over 40% of all tickets with a 1-star CSAT rating. This suggests that process improvements in the billing department and performance optimizations in the product could significantly reduce complaint volume.
  * **Night Shift Inefficiency Quantified**: The night shift exhibited an average Time to Resolution (TTR) that was 22% longer than the day shift, even when controlling for ticket complexity. This points to a need for either increased staffing, better access to subject matter experts, or enhanced knowledge base resources during off-peak hours.
  * **New Hire Learning Curve Mapped**: New agents (tenure \< 3 months) displayed an average First Response Time (FRT) nearly twice the company average. However, their TTR for non-technical issues normalized after the first month. This indicates that initial onboarding should focus heavily on improving familiarity with the support platform and escalation procedures to reduce initial response delays.

-----

## üìÅ Folder Structure

```
project-root/
‚îÇ
‚îú‚îÄ‚îÄ data/                # Contains raw and cleaned datasets in .csv format.
‚îú‚îÄ‚îÄ sql/                 # Contains .sql scripts for metric aggregation and table creation.
‚îú‚îÄ‚îÄ notebooks/           # Jupyter notebooks for exploratory data analysis and text mining.
‚îú‚îÄ‚îÄ scripts/             # Contains standalone Python scripts (.py) for preprocessing and keyword extraction.
‚îú‚îÄ‚îÄ dashboards/          # Contains the final Power BI (.pbix) dashboard files.
‚îú‚îÄ‚îÄ reports/             # Contains written summaries and final presentation documents.
‚îî‚îÄ‚îÄ README.md
```

-----

## ‚ñ∂Ô∏è Run the Project

To reproduce the analysis and view the results, please follow these steps:

1.  **Prepare the Environment**: Ensure you have Python (with pandas and NLTK) and a SQL-enabled database installed. You will also need Power BI Desktop.
2.  **Load Data**: Place the raw dataset into the `data/` directory.
3.  **Data Preprocessing**: Run the `01_preprocessing.py` script from the `scripts/` folder. This will output a cleaned version of the dataset into the `data/` folder.
4.  **Keyword Analysis**: Execute the Jupyter Notebook `02_keyword_analysis.ipynb` in the `notebooks/` directory to generate the keyword mapping table.
5.  **Generate SQL Metrics**: Execute the `03_create_benchmark_table.sql` script in your database client. This will consume the cleaned data and create the final `csat_benchmark` table.
6.  **Visualize Dashboard**: Open the `.pbix` file from the `dashboards/` folder in Power BI. If prompted, refresh the data source to connect to the `csat_benchmark` table in your database.
